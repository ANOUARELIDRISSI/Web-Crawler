# ğŸ“‹ Architecture Report - Web Crawler System

## ğŸ“– Table of Contents

1. [Overview](#overview)
2. [System Architecture](#system-architecture)
3. [File Structure](#file-structure)
4. [Main Components](#main-components)
5. [Workflow](#workflow)
6. [Database](#database)
7. [Data Sources](#data-sources)
8. [AI Service](#ai-service)
9. [User Interface](#user-interface)
10. [Configuration and Deployment](#configuration-and-deployment)
11. [Diagrams](#diagrams)

---

## Overview

**Web Crawler System** is a professional web scraping application with MongoDB storage, Flask UI, and AI-powered analysis. The system allows automatic data collection from various web sources (HTML, RSS, PDF, XML, TXT, dynamic JavaScript pages).

### Key Features

| Feature | Description |
|---------|-------------|
| ğŸŒ Multi-Format | Support for HTML, RSS, PDF, XML, TXT, Dynamic JS |
| ğŸ—„ï¸ MongoDB | NoSQL database with authentication |
| ğŸ³ Docker | Containerized MongoDB deployment |
| â° Scheduler | Configurable automated crawling |
| ğŸ–¼ï¸ Images | Image extraction and display |
| ğŸ¤– AI | Chat and summarization with Hugging Face (DistilBART, GPT-2) |

---

## System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         PRESENTATION LAYER                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  index   â”‚  â”‚ sources  â”‚  â”‚  search  â”‚  â”‚ reports  â”‚  â”‚   ai   â”‚ â”‚
â”‚  â”‚  .html   â”‚  â”‚  .html   â”‚  â”‚  .html   â”‚  â”‚  .html   â”‚  â”‚  .html â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â”‚
â”‚       â”‚             â”‚             â”‚             â”‚             â”‚      â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                            â”‚                                         â”‚
â”‚                      â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”                                   â”‚
â”‚                      â”‚ base.html â”‚  (Base Template)                  â”‚
â”‚                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         APPLICATION LAYER                            â”‚
â”‚                                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                         app.py                                  â”‚ â”‚
â”‚  â”‚  â€¢ Flask Routes (/, /sources, /search, /reports, /ai)          â”‚ â”‚
â”‚  â”‚  â€¢ REST API (/api/sources, /api/crawl, /api/search, /api/ai)   â”‚ â”‚
â”‚  â”‚  â€¢ Custom JSON Encoder (ObjectId, datetime)                    â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                            â”‚                                         â”‚
â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚          â–¼                 â–¼                 â–¼                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚   crawler_    â”‚ â”‚  scheduler.py â”‚ â”‚  ai_service   â”‚              â”‚
â”‚  â”‚ enhanced.py   â”‚ â”‚               â”‚ â”‚     .py       â”‚              â”‚
â”‚  â”‚               â”‚ â”‚  â€¢ Threading  â”‚ â”‚               â”‚              â”‚
â”‚  â”‚  â€¢ HTML/RSS   â”‚ â”‚  â€¢ Schedule   â”‚ â”‚  â€¢ DistilBART â”‚              â”‚
â”‚  â”‚  â€¢ PDF/XML    â”‚ â”‚  â€¢ Frequenciesâ”‚ â”‚  â€¢ GPT-2      â”‚              â”‚
â”‚  â”‚  â€¢ Dynamic JS â”‚ â”‚               â”‚ â”‚  â€¢ Chat/Summaryâ”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚          â”‚                 â”‚                                         â”‚
â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                         â”‚
â”‚                   â–¼                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                       database.py                               â”‚ â”‚
â”‚  â”‚  â€¢ CrawlerDatabase class                                        â”‚ â”‚
â”‚  â”‚  â€¢ CRUD Sources, Crawled Data, Logs                            â”‚ â”‚
â”‚  â”‚  â€¢ Text index for search                                        â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           DATA LAYER                                 â”‚
â”‚                                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                   MongoDB (Docker)                              â”‚ â”‚
â”‚  â”‚                                                                  â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚ â”‚
â”‚  â”‚  â”‚   sources   â”‚  â”‚crawled_data â”‚  â”‚ crawl_logs  â”‚             â”‚ â”‚
â”‚  â”‚  â”‚             â”‚  â”‚             â”‚  â”‚             â”‚             â”‚ â”‚
â”‚  â”‚  â”‚ â€¢ name      â”‚  â”‚ â€¢ source_id â”‚  â”‚ â€¢ source_id â”‚             â”‚ â”‚
â”‚  â”‚  â”‚ â€¢ url       â”‚  â”‚ â€¢ title     â”‚  â”‚ â€¢ url       â”‚             â”‚ â”‚
â”‚  â”‚  â”‚ â€¢ type      â”‚  â”‚ â€¢ content   â”‚  â”‚ â€¢ status    â”‚             â”‚ â”‚
â”‚  â”‚  â”‚ â€¢ selectors â”‚  â”‚ â€¢ images    â”‚  â”‚ â€¢ items     â”‚             â”‚ â”‚
â”‚  â”‚  â”‚ â€¢ frequency â”‚  â”‚ â€¢ timestamp â”‚  â”‚ â€¢ errors    â”‚             â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## File Structure

```
E:\Web Crawling\
â”‚
â”œâ”€â”€ ğŸ“„ app.py                    # Main Flask application
â”œâ”€â”€ ğŸ“„ ai_service.py             # AI service (chat, summarization)
â”œâ”€â”€ ğŸ“„ crawler_enhanced.py       # Enhanced crawler with image support
â”œâ”€â”€ ğŸ“„ crawler.py                # Base crawling engine
â”œâ”€â”€ ğŸ“„ database.py               # MongoDB access layer
â”œâ”€â”€ ğŸ“„ default_sources.py        # 22 pre-configured sources
â”œâ”€â”€ ğŸ“„ scheduler.py              # Task scheduler
â”‚
â”œâ”€â”€ ğŸ“„ docker-compose.yml        # Docker MongoDB configuration
â”œâ”€â”€ ğŸ“„ requirements.txt          # Python dependencies
â”œâ”€â”€ ğŸ“„ .env.template             # Environment variables template
â”œâ”€â”€ ğŸ“„ .gitignore                # Git ignored files
â”‚
â”œâ”€â”€ ğŸ“ templates/                # Jinja2 HTML templates
â”‚   â”œâ”€â”€ base.html               # Base template
â”‚   â”œâ”€â”€ index.html              # Dashboard
â”‚   â”œâ”€â”€ sources.html            # Source management
â”‚   â”œâ”€â”€ search.html             # Search with filters
â”‚   â”œâ”€â”€ reports.html            # Reports and charts
â”‚   â”œâ”€â”€ ai.html                 # AI Assistant
â”‚   â”œâ”€â”€ 404.html                # 404 error page
â”‚   â””â”€â”€ 500.html                # 500 error page
â”‚
â”œâ”€â”€ ğŸ“ scripts/                  # Utility scripts
â”‚   â”œâ”€â”€ demo.py                 # Demonstration
â”‚   â”œâ”€â”€ mongo-init.js           # MongoDB initialization
â”‚   â”œâ”€â”€ setup_mongodb.py        # MongoDB setup
â”‚   â””â”€â”€ fix_mongodb*.py         # Repair scripts
â”‚
â”œâ”€â”€ ğŸ“ tests/                    # Automated tests
â”‚   â”œâ”€â”€ test_ai.py              # AI service tests
â”‚   â”œâ”€â”€ test_crawl.py           # Crawling tests
â”‚   â”œâ”€â”€ test_mongodb_connection.py
â”‚   â”œâ”€â”€ test_sources.py         # Sources tests
â”‚   â””â”€â”€ verify_system.py        # System verification
â”‚
â”œâ”€â”€ ğŸ“ docs/                     # Documentation
â”‚   â””â”€â”€ ARCHITECTURE_REPORT.md  # This document
â”‚
â””â”€â”€ ğŸ“ venv/                     # Python virtual environment
```

---

## Main Components

### 1. app.py - Flask Application

The heart of the application, responsible for:

```python
# Initialization
db = CrawlerDatabase()           # MongoDB connection
crawler = EnhancedWebCrawler(db) # Crawling engine

# Main Routes
@app.route('/')          â†’ index.html      # Dashboard
@app.route('/sources')   â†’ sources.html    # Source management
@app.route('/search')    â†’ search.html     # Search
@app.route('/reports')   â†’ reports.html    # Reports
@app.route('/ai')        â†’ ai.html         # AI Assistant

# REST API
POST /api/sources/add      # Add a source
DELETE /api/sources/<id>   # Delete a source
POST /api/crawl            # Start a crawl
POST /api/search           # Search data
POST /api/ai/chat          # Chat with AI
POST /api/ai/summarize     # Summarize data
```

### 2. database.py - Data Access Layer

```python
class CrawlerDatabase:
    # MongoDB Collections
    self.sources       # Source configuration
    self.crawled_data  # Collected data
    self.crawl_logs    # Execution logs
    
    # Main Methods
    add_source()           # Add source
    update_source()        # Update source
    delete_source()        # Delete source
    store_crawled_data()   # Store data
    bulk_store_data()      # Bulk storage
    search_by_keyword()    # Text search
    get_statistics()       # Statistics
    log_crawl()            # Logging
```

### 3. crawler.py & crawler_enhanced.py - Crawling Engine

```python
class WebCrawler:
    # Supported crawling types
    _crawl_html()     # Static HTML pages
    _crawl_dynamic()  # JavaScript pages (Selenium)
    _crawl_rss()      # RSS/Atom feeds
    _crawl_pdf()      # PDF documents
    _crawl_xml()      # XML files
    _crawl_txt()      # Text files

class EnhancedWebCrawler(WebCrawler):
    # Extension with image support
    _make_absolute_url()  # Relative â†’ absolute URLs
    # Image extraction from containers
```

### 4. ai_service.py - Artificial Intelligence Service

```python
class AIService:
    # Models used
    summarizer = DistilBART  # Text summarization
    chat_model = GPT-2       # Conversation
    
    # Features
    summarize_text()         # Summarize text
    summarize_data_items()   # Summarize multiple items
    chat()                   # Contextual conversation
    analyze_data()           # Statistical analysis
```

### 5. scheduler.py - Scheduler

```python
class CrawlerScheduler:
    # Supported frequencies
    "hourly"   â†’ Every hour
    "daily"    â†’ Every day
    "weekly"   â†’ Every week
    "monthly"  â†’ Every 30 days
    "N"        â†’ Every N minutes (custom)
    
    # Methods
    schedule_source()      # Schedule a source
    schedule_all_sources() # Schedule all sources
    start()                # Start in background
    stop()                 # Stop the scheduler
```

---

## Workflow

### Crawling Workflow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     User     â”‚    â”‚    Flask API    â”‚    â”‚     Crawler      â”‚
â”‚              â”‚    â”‚                 â”‚    â”‚                  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                     â”‚                      â”‚
       â”‚  POST /api/crawl    â”‚                      â”‚
       â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚                      â”‚
       â”‚                     â”‚                      â”‚
       â”‚                     â”‚  crawl_source()      â”‚
       â”‚                     â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚
       â”‚                     â”‚                      â”‚
       â”‚                     â”‚                      â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚                     â”‚                      â”‚â”€>â”‚   Website   â”‚
       â”‚                     â”‚                      â”‚  â”‚   (HTTP)    â”‚
       â”‚                     â”‚                      â”‚<â”€â”‚             â”‚
       â”‚                     â”‚                      â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                     â”‚                      â”‚
       â”‚                     â”‚                      â”‚  Parse HTML/RSS/PDF
       â”‚                     â”‚                      â”‚  Extract data
       â”‚                     â”‚                      â”‚
       â”‚                     â”‚                      â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚                     â”‚                      â”‚â”€>â”‚   MongoDB   â”‚
       â”‚                     â”‚                      â”‚  â”‚ store_data()â”‚
       â”‚                     â”‚                      â”‚<â”€â”‚             â”‚
       â”‚                     â”‚                      â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                     â”‚                      â”‚
       â”‚                     â”‚  {success, items}    â”‚
       â”‚                     â”‚<â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
       â”‚                     â”‚                      â”‚
       â”‚  JSON Response      â”‚                      â”‚
       â”‚<â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚                      â”‚
       â”‚                     â”‚                      â”‚
```

### Search Workflow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     User     â”‚    â”‚    Flask API    â”‚    â”‚     MongoDB      â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                     â”‚                      â”‚
       â”‚  POST /api/search   â”‚                      â”‚
       â”‚  {keyword: "python"}â”‚                      â”‚
       â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚                      â”‚
       â”‚                     â”‚                      â”‚
       â”‚                     â”‚ search_by_keyword()  â”‚
       â”‚                     â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚
       â”‚                     â”‚                      â”‚
       â”‚                     â”‚                      â”‚ $text index
       â”‚                     â”‚                      â”‚ search
       â”‚                     â”‚                      â”‚
       â”‚                     â”‚   [results...]       â”‚
       â”‚                     â”‚<â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
       â”‚                     â”‚                      â”‚
       â”‚  JSON Results       â”‚                      â”‚
       â”‚<â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚                      â”‚
```

### AI Workflow (Chat/Summary)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     User     â”‚    â”‚    Flask API    â”‚    â”‚   AI Service     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                     â”‚                      â”‚
       â”‚  POST /api/ai/chat  â”‚                      â”‚
       â”‚  {message: "..."}   â”‚                      â”‚
       â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚                      â”‚
       â”‚                     â”‚                      â”‚
       â”‚                     â”‚ 1. get_recent_data() â”‚
       â”‚                     â”‚    (context)         â”‚
       â”‚                     â”‚                      â”‚
       â”‚                     â”‚ 2. chat(msg, context)â”‚
       â”‚                     â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚
       â”‚                     â”‚                      â”‚
       â”‚                     â”‚                      â”‚ GPT-2 generates
       â”‚                     â”‚                      â”‚ response
       â”‚                     â”‚                      â”‚
       â”‚                     â”‚   AI response        â”‚
       â”‚                     â”‚<â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
       â”‚                     â”‚                      â”‚
       â”‚  JSON Response      â”‚                      â”‚
       â”‚<â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚                      â”‚
```

---

## Database

### MongoDB Configuration

```yaml
# docker-compose.yml
services:
  mongodb:
    image: mongo:7.0
    container_name: web_crawler_mongodb
    ports:
      - "27017:27017"
    environment:
      MONGO_INITDB_ROOT_USERNAME: admin
      MONGO_INITDB_ROOT_PASSWORD: AdminPass2026
      MONGO_INITDB_DATABASE: web_crawler
    volumes:
      - mongodb_data:/data/db
      - ./scripts/mongo-init.js:/docker-entrypoint-initdb.d/mongo-init.js
```

### Collection Schemas

#### `sources` Collection
```json
{
  "_id": ObjectId,
  "name": "GitHub Blog RSS",
  "url": "https://github.blog/feed/",
  "type": "rss",                    // html, rss, pdf, xml, txt, dynamic
  "description": "GitHub's official blog",
  "category": "Technology",
  "selectors": {                    // For HTML
    "container": ".post",
    "title": ".post-title",
    "content": ".post-body"
  },
  "frequency": "daily",             // hourly, daily, weekly, monthly
  "schedule_time": "00:00",
  "max_items": 50,
  "status": "active",
  "created_at": ISODate,
  "updated_at": ISODate
}
```

#### `crawled_data` Collection
```json
{
  "_id": ObjectId,
  "source_id": "source_ObjectId",
  "source_url": "https://...",
  "type": "html",
  "title": "Article Title",
  "content": "Article content...",
  "data": {
    "title": "...",
    "author": "...",
    "custom_field": "..."
  },
  "images": [
    {"url": "https://...", "alt": "Image description"}
  ],
  "link": "https://...",            // For RSS
  "published": "2026-01-15",        // For RSS
  "pages": 10,                      // For PDF
  "timestamp": ISODate
}
```

#### `crawl_logs` Collection
```json
{
  "_id": ObjectId,
  "source_id": "source_ObjectId",
  "url": "https://...",
  "status": "success",              // success, error, no_data
  "items_collected": 25,
  "errors": [],
  "timestamp": ISODate
}
```

### MongoDB Indexes

```javascript
// Text index for full-text search
crawled_data.createIndex({ content: "text", title: "text" })

// Index for queries by source and date
crawled_data.createIndex({ source_id: 1, timestamp: 1 })

// Unique index to avoid duplicate sources
sources.createIndex({ url: 1 }, { unique: true })
```

---

## Data Sources

### Pre-configured Sources (22 sources)

The `default_sources.py` file contains 22 sources organized by category:

| Category | Sources | Type |
|----------|---------|------|
| **Technology** | Python Blog, GitHub Blog, Reddit Programming, Dev.to, Stack Overflow, Medium | HTML, RSS |
| **News** | TechCrunch, Hacker News, BBC News, Reuters, Wired | RSS |
| **Science** | NASA Breaking News | RSS |
| **PDF Documents** | arXiv Papers, WHO Reports | PDF, HTML |
| **Books** | Project Gutenberg, Open Library, Internet Archive, Standard Ebooks | HTML |
| **Images** | NASA Image of the Day, Flickr Explore | RSS, HTML |

### Supported Source Types

| Type | Description | Library |
|------|-------------|---------|
| `html` | Static web pages | BeautifulSoup |
| `dynamic` | JavaScript pages | SeleniumBase |
| `rss` | RSS/Atom feeds | feedparser |
| `pdf` | PDF documents | PyPDF2 |
| `xml` | XML files | BeautifulSoup (xml) |
| `txt` | Text files | requests |

### CSS Selector Configuration

```python
{
    "name": "Example Site",
    "url": "https://example.com",
    "type": "html",
    "selectors": {
        "container": ".article",      # Parent element (required)
        "title": "h2.title",          # Title selector
        "content": ".body p",         # Content selector
        "author": ".meta .author",    # Custom fields
        "date": ".meta .date"
    },
    "max_items": 50                    # Items limit
}
```

---

## AI Service

### Models Used

| Model | Usage | Size |
|-------|-------|------|
| **DistilBART** (`sshleifer/distilbart-cnn-12-6`) | Text summarization | ~1.2 GB |
| **GPT-2** (`gpt2`) | Chat/Generation | ~500 MB |

### Features

1. **Text Summarization**
   - Input: Raw text (max 1000 characters)
   - Output: Summary (30-130 words)
   
2. **Multi-item Summarization**
   - Combines content from multiple items
   - Generates a global summary

3. **Contextual Chat**
   - Uses recent data as context
   - Answers questions about crawled data
   
4. **Data Analysis**
   - Statistics by content type
   - Count by source
   - Temporal distribution

### Rule-Based Fallback

If AI models fail, a rule-based system takes over:

```python
if "how many" in message â†’ Count response
if "summarize" in message â†’ Simple summary
if "what" in message     â†’ Data description
else                     â†’ Generic help message
```

---

## User Interface

### HTML Templates

| Template | Route | Description |
|----------|-------|-------------|
| `base.html` | - | Common layout, navigation, CSS/JS |
| `index.html` | `/` | Dashboard with statistics |
| `sources.html` | `/sources` | Source CRUD management |
| `search.html` | `/search` | Search with type filters |
| `reports.html` | `/reports` | Charts and analytics |
| `ai.html` | `/ai` | AI chat interface |
| `404.html` | - | 404 error page |
| `500.html` | - | 500 error page |

### UI Features

- **Responsive Design**: Adaptive for mobile/desktop
- **SVG Icons**: Professional design without emojis
- **Real-time Stats**: Updated metrics
- **Image Gallery**: Grid layout with modal preview
- **PDF Viewer**: Direct links to PDFs
- **Type Filtering**: Filter by content type

---

## Configuration and Deployment

### Environment Variables (.env)

```bash
# MongoDB
MONGODB_URI=mongodb://crawler_admin:password@localhost:27017/web_crawler?authSource=admin
MONGODB_DATABASE=web_crawler

# Flask
FLASK_ENV=development
FLASK_DEBUG=True
SECRET_KEY=your-secret-key

# AI (optional)
OPENAI_API_KEY=your-api-key
AI_MODEL=gpt-3.5-turbo

# Crawler
CRAWLER_USER_AGENT=WebCrawler/1.0
CRAWLER_DELAY=1
CRAWLER_TIMEOUT=30

# Scheduler
SCHEDULER_ENABLED=True
CRAWL_INTERVAL_HOURS=24
```

### Quick Start

```bash
# 1. Activate virtual environment
.\venv\Scripts\activate

# 2. Install dependencies
pip install -r requirements.txt

# 3. Start MongoDB (Docker)
docker-compose up -d

# 4. Launch application
python app.py

# 5. Access dashboard
# http://localhost:5000
```

### Docker Commands

```bash
# Start MongoDB
docker-compose up -d

# Check status
docker ps

# View logs
docker logs web_crawler_mongodb

# Stop
docker-compose down

# Delete data
docker-compose down -v
```

---

## Diagrams

### Sequence Diagram - Full Crawl

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  User  â”‚     â”‚  Flask  â”‚     â”‚ Crawler  â”‚     â”‚ Website  â”‚     â”‚ MongoDB â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
    â”‚               â”‚               â”‚                â”‚                â”‚
    â”‚ Click "Crawl" â”‚               â”‚                â”‚                â”‚
    â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚               â”‚                â”‚                â”‚
    â”‚               â”‚               â”‚                â”‚                â”‚
    â”‚               â”‚ crawl_source()â”‚                â”‚                â”‚
    â”‚               â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚                â”‚                â”‚
    â”‚               â”‚               â”‚                â”‚                â”‚
    â”‚               â”‚               â”‚  GET request   â”‚                â”‚
    â”‚               â”‚               â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚                â”‚
    â”‚               â”‚               â”‚                â”‚                â”‚
    â”‚               â”‚               â”‚  HTML response â”‚                â”‚
    â”‚               â”‚               â”‚<â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚                â”‚
    â”‚               â”‚               â”‚                â”‚                â”‚
    â”‚               â”‚               â”‚ Parse & Extractâ”‚                â”‚
    â”‚               â”‚               â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚                â”‚
    â”‚               â”‚               â”‚        â”‚       â”‚                â”‚
    â”‚               â”‚               â”‚<â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚                â”‚
    â”‚               â”‚               â”‚                â”‚                â”‚
    â”‚               â”‚               â”‚    store_data()â”‚                â”‚
    â”‚               â”‚               â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚
    â”‚               â”‚               â”‚                â”‚                â”‚
    â”‚               â”‚               â”‚    log_crawl() â”‚                â”‚
    â”‚               â”‚               â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚
    â”‚               â”‚               â”‚                â”‚                â”‚
    â”‚               â”‚   {success}   â”‚                â”‚                â”‚
    â”‚               â”‚<â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚                â”‚                â”‚
    â”‚               â”‚               â”‚                â”‚                â”‚
    â”‚  Show results â”‚               â”‚                â”‚                â”‚
    â”‚<â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚               â”‚                â”‚                â”‚
    â”‚               â”‚               â”‚                â”‚                â”‚
```

### Component Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              WEB CRAWLER SYSTEM                          â”‚
â”‚                                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                         FRONTEND (Templates)                     â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚    â”‚
â”‚  â”‚  â”‚Dashboardâ”‚ â”‚ Sources â”‚ â”‚ Search  â”‚ â”‚ Reports â”‚ â”‚   AI    â”‚   â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                    â”‚                                     â”‚
â”‚                                    â–¼                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                         BACKEND (Python)                         â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚    â”‚
â”‚  â”‚  â”‚                     app.py (Flask)                        â”‚   â”‚    â”‚
â”‚  â”‚  â”‚           Routes + API + JSON Encoder                     â”‚   â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚    â”‚
â”‚  â”‚           â”‚                â”‚                â”‚                    â”‚    â”‚
â”‚  â”‚           â–¼                â–¼                â–¼                    â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚    â”‚
â”‚  â”‚  â”‚   Crawler    â”‚ â”‚  Scheduler   â”‚ â”‚  AI Service  â”‚             â”‚    â”‚
â”‚  â”‚  â”‚  Enhanced    â”‚ â”‚              â”‚ â”‚              â”‚             â”‚    â”‚
â”‚  â”‚  â”‚              â”‚ â”‚  â€¢ schedule  â”‚ â”‚  â€¢ DistilBARTâ”‚             â”‚    â”‚
â”‚  â”‚  â”‚ â€¢ HTML/RSS   â”‚ â”‚  â€¢ threading â”‚ â”‚  â€¢ GPT-2     â”‚             â”‚    â”‚
â”‚  â”‚  â”‚ â€¢ PDF/XML    â”‚ â”‚              â”‚ â”‚              â”‚             â”‚    â”‚
â”‚  â”‚  â”‚ â€¢ Selenium   â”‚ â”‚              â”‚ â”‚              â”‚             â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚    â”‚
â”‚  â”‚           â”‚                â”‚                                     â”‚    â”‚
â”‚  â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚    â”‚
â”‚  â”‚                            â–¼                                     â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚    â”‚
â”‚  â”‚  â”‚                    database.py                            â”‚   â”‚    â”‚
â”‚  â”‚  â”‚              CrawlerDatabase (PyMongo)                    â”‚   â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                    â”‚                                     â”‚
â”‚                                    â–¼                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                         DATABASE (Docker)                        â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚    â”‚
â”‚  â”‚  â”‚                    MongoDB 7.0                            â”‚   â”‚    â”‚
â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚   â”‚    â”‚
â”‚  â”‚  â”‚  â”‚ sources  â”‚  â”‚ crawled_data â”‚  â”‚ crawl_logs â”‚          â”‚   â”‚    â”‚
â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚   â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Summary

The **Web Crawler System** is a complete and modular solution for:

1. âœ… **Collecting** data from various web sources
2. âœ… **Storing** in a structured way in MongoDB
3. âœ… **Scheduling** automatic crawls
4. âœ… **Searching** with full-text index
5. âœ… **Analyzing** with AI (summary, chat)
6. âœ… **Visualizing** via a modern web interface

### Key Technologies

| Category | Technologies |
|----------|--------------|
| **Backend** | Python 3.8+, Flask, PyMongo |
| **Crawling** | BeautifulSoup, SeleniumBase, feedparser, PyPDF2 |
| **Database** | MongoDB 7.0, Docker |
| **AI** | Hugging Face Transformers (DistilBART, GPT-2) |
| **Frontend** | Jinja2, HTML5, CSS3, JavaScript |
| **Scheduling** | schedule, threading |

---

*Document generated on January 16, 2026*
